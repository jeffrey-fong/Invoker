# Invoker

<div align="center">

<img width="300" height="300" src="invoker.png">

***The one who calls upon... Functions!***

</div>

Invoker is a suite of large language models based on Llama-2 and is finetuned to plan between calling functions and providing responses directly. Currently, we have released the 13B version and there are plans for the 7B and 34B versions to be trained and released in the future.

## Installation & Usage

The usage of Invoker follows exactly like OpenAI's function calling. Simply install the required dependencies:

```shell
pip install -r requirements.txt
```

#### Launching the Server

Kick-start the FastAPI server. You can indicate the model via environment variables. The list of models are indicated [here](#download).

```shell
EXPORT INVOKER_MODEL_NAME_OR_PATH=jeffrey-fong/Invoker-13b
uvicorn server_api:app
```

There are plans to set up accelerated servers based on [ExLlama](https://github.com/turboderp/exllama) and/or [ExLlamaV2](https://github.com/turboderp/exllamav2) that can work with GPTQ-based models. Stay tuned for more updates!

#### Inference

Inference can then be performed exactly like OpenAI function-calling. Provide the chat and the functions in the `messages` and `functions` arguments respectively. Invoker also supports the following generation hyperparameters:

- `temperature: float = 0.7` Accepts values between 0.0 and 1.0. Defaults to 0.7 if the temperature is not passed in.
- `top_p: float = 1.0` Accepts values between 0.0 and 1.0. Defaults to 1.0 if the top_p is not passed in.

```python
import openai

openai.api_base = "http://localhost:8000"
openai.api_key = "test"

response = openai.ChatCompletion.create(
    model="jeffrey-fong/invoker-13b",
    messages=[{"role": "user", "content": "What time is it in Singapore?"}],
    functions=[{
        "name": "get_time",
        "description": "Get the current time",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g. New York City, NY"
                },
                "format": {
                  "type": "string",
                  "enum": ["12-hour", "24-hour"]
                }
            },
            "required": ["location"],
        },
    }]
)
response_message = response["choices"][0]["message"]
```

The model can choose to call a function; if so, the content will be a stringified JSON object indicating a function call with the function name and arguments generated by the model (note: the model may generate invalid JSON or hallucinate parameters). To allow the model to summarize the results of the function response, parse the string into JSON in your code, and call your function with the provided arguments if they exist. Perform another inference with the model after appending the function response as a new message.

Using the above example again,

```python
if response_message.get("function_call"):
  available_functions = {"get_time": get_time}
  function_name = response_message["function_call"]["name"]
  function_to_call = available_functions[function_name]
  function_args = json.loads(response_message["function_call"]["arguments"])
  function_response = function_to_call(
      location=function_args.get("location"),
      unit=function_args.get("format"),
  )
  messages.append(response_message)
  messages.append(
      {
          "role": "function",
          "name": function_name,
          "content": function_response,
      }
  )
  second_response = openai.ChatCompletion.create(
      model="jeffrey-fong/invoker-13b",
      messages=messages,
  )
  print(second_response["choices"][0]["message"])
```


#### Model Download
| Model  |  Link | Version |
| ------------- | ------------- |------------- |
| Invoker-13B  | [Huggingface Repo](https://huggingface.co/jeffrey-fong/invoker-13b) |v1.0|
| Invoker-13B-GPTQ  | Coming Soon |v1.0|
| Invoker-7B  | Coming Soon |v1.0|
| Invoker-34B  | Coming Soon |v1.0|

## Training

## To-Dos

- [ ] Work on validating function names, descriptions, etc. Just like OpenAI's function calling
- [ ] Quantize 13B model
- [ ] Work on GPTQ-based servers ([ExLlama](https://github.com/turboderp/exllama) and/or [ExLlamaV2](https://github.com/turboderp/exllamav2))
- [ ] Train 7B Llama-2 model and 34B CodeLlama model
- [ ] Investigate ways to evaluate function calling

## Citation

If this work is helpful, please kindly cite as:

```bibtex
@Misc{invoker-function-calling,
  title = {Invoker: The one who calls upon functions - Function-Calling Language Model},
  author = {jeffrey-fong},
  howpublished = {\url{https://github.com/jeffrey-fong/Invoker}},
  year = {2023}
}
```